<!DOCTYPE html>
<html lang="en">

  <head>
      <title>John Lalor</title>
      <link href='http://fonts.googleapis.com/css?family=Open+Sans:400italic,600italic,700italic,400,600,700' rel='stylesheet' type='text/css' />
      <link href='http://fonts.googleapis.com/css?family=Merriweather:300' rel='stylesheet' type='text/css'/>
      <link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:200,400,700' rel='stylesheet' type='text/css'/>
      <link rel="stylesheet" type="text/css" href="./theme/css/icons.css"/>
      <link rel="stylesheet" type="text/css" href="./theme/css/styles.css"/>
      <meta charset="utf-8" />
  </head>

  <body id="index">
    <!-- header -->
    <header class="siteheader">
      <!-- site image -->
        <div class= "siteimage">
          <a class="nodec" href=images/headshot_cropped.jpg>
            <img width="200" height="200" src=images/headshot_cropped.jpg>
          </a>
        </div>

      <div class = "sitebanner">
        <h1><a class="sitetitle nodec" href=".">John Lalor</a></h1>
        <h3 class ="sitesubtitle"></h3>
        <!-- nav -->
        <nav class="menu">
          <ul>
            <!-- menu items-->
              <li><a class="nodec" href="/">home</a></li>
              <li><a class="nodec" href="/blog_index.html">blog</a></li>
              <li><a class="nodec" href="/pdfs/cv.pdf">cv</a></li>
            <!--pages-->
            <!-- services icons -->
              <li><a class="nodec icon-mail-alt" href="mailto:lalor@cs.umass.edu"></a></li>
              <li><a class="nodec icon-github" href="https://github.com/jplalor"></a></li>
          </ul>
        </nav>
      </div> <!-- sitebanner -->
    </header>

    <!-- content -->

<section class="content">

  <h3 class="posttitle">
    <a class="nodec" href="/info-theory-part-3.html" rel="bookmark" title="Permalink to StatNLP: Essential Information Theory, Part 3">
      StatNLP: Essential Information Theory, Part 3
    </a>
  </h3>

  <div class="postinfo">
    <p class="published" title="2018-03-28T00:00:00-04:00">
      Wed 28 March 2018
    </p>

  </div><!-- .postinfo -->

  <div class="article">
    <p>After a bit of a hiatus, it's time to wrap up the Information Theory introduction. In this post I'm going to talk about Mutual Information and KL-Divergence. I'll look at why these two are important, how we get them from what we already know, and what we can do with them (with an example or two for good measure). After this post we can hit the ground running with the StatNLP book, woohoo!</p>
<h2>Mutual Information</h2>
<p>At the end of the previous post, we looked at the relationship between joint entropy and conditional entropy. Recall that <span class="math">\(H(X,Y) = H(X) + H(Y \vert X)\)</span>. In this formula, we can reverse <span class="math">\(X\)</span> and <span class="math">\(Y\)</span> to get <span class="math">\(H(X,Y) = H(Y) + H(X \vert Y)\)</span>. The two are equivalent. If we rearrange those formulas we can derive the formula for <em>mutual information:</em></p>
<div class="math">$$
\begin{align}
H(X) + H(Y \vert X) &amp;= H(Y) + H(X \vert Y) \\
H(X) - H(X \vert Y) &amp;= H(Y) - H(Y \vert X) \\
&amp;= I(X;Y)
\end{align}
$$</div>
<p>
Mutual information describes the reduction in uncertainty of one random variable that you get if you know another random variable. For example if <span class="math">\(X\)</span> is a random variable indicating whether it is currently raining or not, and <span class="math">\(Y\)</span> is a random variable indicating whether the ground or not is wet, if you know that the ground is wet (<span class="math">\(Y=1\)</span>), that will reduce the amount of uncertainty in determining whether it is raining (<span class="math">\(X\)</span>).</p>
<p>How do you calculate mutual information? From Equation 2.36 in StatNLP:</p>
<div class="math">$$
\begin{align}
I(X;Y) &amp;= H(X) - H(X \vert Y) \\
&amp;= H(X) + H(Y) - H(X,Y)  \textit{ by the chain rule} \\
&amp;= \sum_x p(x) \log \frac{1}{p(x)} + \sum_y p(y) \log \frac{1}{p(y)} + \sum_{x,y} p(x,y) \log p(x,y) \\
&amp;= \sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}
\end{align}
$$</div>
<p>In the case where two random variables are independent, then <span class="math">\(I(X;Y) = 0\)</span>. Why? Because <span class="math">\(H(X \vert Y) = H(X)\)</span>, so we have <span class="math">\(I(X;Y) = H(X) - H(X) = 0\)</span>. This makes sense since mutual information is measuring the reduction in uncertainty for one variable when you know another one. If you know <span class="math">\(Y\)</span>, but it is independent of <span class="math">\(X\)</span>, there is no reduction in uncertainty for <span class="math">\(X\)</span>. </p>
<h2>KL-Divergence</h2>
<p>The next definition is for Kullback-Leibler divergence, or KL-divergence. KL-divergence measures how far one probability distribution is from another:</p>
<div class="math">$$
D(p \vert \vert q) = \sum_{x \in X} p(x) \log \frac{p(x)}{q(x)}
$$</div>
<p>
It's worth noting that KL-divergence is not a distance metric since it is not symmetric, <span class="math">\(D(p \vert \vert q) \neq D(q \vert \vert p)\)</span>. It's useful for determining how good of an estimate q is for p. And looking at our equation for mutual information, we see that it can be expressed as a KL-divergence: <span class="math">\(I(X;Y) = D(p(x,y) \vert \vert p(x)p(y))\)</span>. This shows us that mutual information is a way to quantify the dependence of a joint distribution. </p>
<h2>Cross Entropy</h2>
<p>Imagine that you have some random variable <span class="math">\(X\)</span> with some true probability distribution <span class="math">\(p(x)\)</span>. You don't know what <span class="math">\(p\)</span> is, but you have this other distribution <span class="math">\(q\)</span>, and you want to know how good of a job it does at approximating <span class="math">\(p(x)\)</span>. To answer this question requires <em>cross entropy:</em> <span class="math">\(H(X, q) = H(X) + D(p \vert \vert q)\)</span>. Now wait a minute. The formula for cross entropy still requires that we know <span class="math">\(p(x)\)</span>. Let's rearrange some terms:</p>
<div class="math">$$
\begin{align}
H(X, q) &amp;= H(X) + D(p \vert \vert q) \\
&amp;= - \sum_x p(x) \log p(x) + \sum_x p(x) \log \frac{p(x)}{q(x)} \\
&amp;= - \sum_x p(x) \log p(x) + \sum_x p(x) \log p(x) + \sum_x p(x) \log \frac{1}{q(x)} \\
&amp;= \sum_x p(x) \frac{1}{\log q(x)} \\
&amp;= \mathbb{E}_p [\log \frac{1}{q(x)}]
\end{align}
$$</div>
<p>
We can calculate this if we know <span class="math">\(q\)</span>.</p>
<h2>Wrapping Up</h2>
<p>These last few posts had a lot of definitions and math in them, but hopefully they are helpful for defining some key concepts in Information Theory. Entropy is a useful tool for machine learning and NLP, and upcoming posts will be using these concepts.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div><!-- .content -->

</section>


    <!-- footer -->
    <footer>
      <p>
        Â© John Lalor, license <a href=""> </a>
        unless otherwise noted.
        Generated by <a href= "http://docs.getpelican.com/">Pelican</a> with
        <a href="http://github.com/porterjamesj/crowsfoot">crowsfoot</a> theme.
      </p>
    </footer>
  </body>
</html>